from flask import Flask, request, jsonify
from transformers import pipeline, AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import uuid, os
from flask_cors import CORS
import time
import torch
from werkzeug.utils import secure_filename
import paramiko
import json
import getpass
import markdown

# ===== CONFIG =====
KEY_PATH = "/home/d1l1th1um/Desktop/id_rsa"
CMS_HOST = "cms-ssh.sc.imr.tohoku.ac.jp"
GPU_HOST = "gpu.sc.imr.tohoku.ac.jp"
USER = "inomar01"
REMOTE_DIR = "/home/inomar01/Hieu"
REMOTE_IMAGE = f"{REMOTE_DIR}/upload"
LOCAL_IMAGE = "/home/d1l1th1um/Desktop/demo/vqa-image"

app = Flask(__name__)
CORS(app, origins=["http://10.102.196.113"], supports_credentials=True) # Allow all origins for API server
# Folders
UPLOAD_FOLDER = 'static/uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Load models
skin_cancer_model = pipeline("image-classification", model="Anwarkh1/Skin_Cancer-Image_Classification")
pneumonia_model = pipeline("image-classification", model="lxyuan/vit-xray-pneumonia-classification")
breast_cancer_model = pipeline("image-classification", model="Falah/vit-base-breast-cancer")
covid19_model = pipeline("image-classification", model="DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans")
brain_tumor_model_vit = pipeline("image-classification", model="DunnBC22/vit-base-patch16-224-in21k_brain_tumor_diagnosis")
brain_tumor_model_resnet50 = pipeline("image-classification", model="Alia-Mohammed/resnet-50-finetuned-brain-tumor")
# Vision qa models

#terminal auth: hf-auth-login
#model_id = "google/medgemma-4b-it"
#
#model = AutoModelForImageTextToText.from_pretrained(
#    model_id,
#    torch_dtype=torch.bfloat16,
#    device_map="auto",
#)
#processor = AutoProcessor.from_pretrained(model_id)

def normalize_results(results):
    # N·∫øu l√† dict duy nh·∫•t ‚Üí b·ªçc v√†o list
    if isinstance(results, dict):
        return [results]
    return results


def parse_model_names(model_name_str):
    if not model_name_str:
        return []

    # T√°ch theo d·∫•u ph·∫©y v√† l√†m s·∫°ch
    models = [m.strip() for m in model_name_str.split(",")]

    # Lo·∫°i b·ªè chu·ªói r·ªóng + lo·∫°i tr√πng nhau, gi·ªØ ƒë√∫ng th·ª© t·ª± xu·∫•t hi·ªán
    unique_models = []
    seen = set()

    for m in models:
        if m and m not in seen:
            unique_models.append(m)
            seen.add(m)

    return unique_models


# ----- Diagnosis -----
@app.route("/diagnose", methods=["POST"])
def diagnose():
    model_names = request.form.get("model")
    file = request.files.get("file")

    print(file.filename)

    if not model_names or not file:
        return jsonify({"error":"Thi·∫øu model ho·∫∑c file ·∫£nh"}),400

    try:
        image = Image.open(file)
    except Exception as e:
        return jsonify({"error":f"L·ªói ƒë·ªçc ·∫£nh: {e}"}),400

    filename = f"{uuid.uuid4()}_{secure_filename(file.filename)}"
    image.save(os.path.join(UPLOAD_FOLDER, filename))

    models = parse_model_names(model_names)

    final_results = []

    model_map = {
        "skin_cancer_vit": "Ung th∆∞ da",
        "pneumonia_vit": "Vi√™m ph·ªïi",
        "covid19_vit": "Covid-19",
        "breast_cancer_vit": "Ung th∆∞ v√∫",
        "brain_tumor_vit": "U n√£o - A",
        "brain_tumor_resnet": "U n√£o - B"
    }

    for model_name in models:
        if model_name=="skin_cancer_vit":
            results = normalize_results(skin_cancer_model(image))
            label_map = {
                "melanocytic_Nevi": "N·ªët ru·ªìi l√†nh t√≠nh",
                "benign_keratosis-like_lesions": "T·ªïn th∆∞∆°ng s·ª´ng l√†nh t√≠nh",
                "melanoma": "U √°c t√≠nh",
                "actinic_keratoses": "D√†y s·ª´ng √°nh s√°ng",
                "basal_cell_carcinoma": "Ung th∆∞ bi·ªÉu m√¥ t·∫ø b√†o ƒë√°y"
            }

        elif model_name=="pneumonia_vit":
            results = normalize_results(pneumonia_model(image))
            label_map = {
                "NORMAL": "Ph·ªïi b√¨nh th∆∞·ªùng",
                "PNEUMONIA": "Vi√™m ph·ªïi"
            }

        elif model_name=="covid19_vit":
            results = normalize_results(covid19_model(image))
            label_map = {
                "CT_COVID": "H√¨nh ·∫£nh CT c√≥ d·∫•u hi·ªáu COVID-19",
                "CT_NonCOVID": "H√¨nh ·∫£nh CT kh√¥ng c√≥ d·∫•u hi·ªáu COVID-19"
            }

        elif model_name=="breast_cancer_vit":
            results = normalize_results(breast_cancer_model(image))
            label_map = {
                "class0": "Kh√¥ng b·ªã ung th∆∞ v√∫",
                "class1": "B·ªã ung th∆∞ v√∫"
            }
        elif model_name=='brain_tumor_vit':
            results = normalize_results(brain_tumor_model_vit(image))
            label_map = {
                "yes": "B·ªã u n√£o",
                "no": "Kh√¥ng b·ªã u n√£o"
            }
        elif model_name=='brain_tumor_resnet':
            results = normalize_results(brain_tumor_model_resnet50(image))
            label_map = {
                "meningioma": "U m√†ng n√£o",
                "pituitary": "U tuy·∫øn y√™n",
                "glioma": "U t·∫ø b√†o th·∫ßn kinh ƒë·ªám",
                "notumor": "Kh√¥ng c√≥ kh·ªëi u"
            }
        else:
            return jsonify({"error":"Model kh√¥ng h·ª£p l·ªá"}),400

        # √Åp d·ª•ng filter mapping
        filtered_results = []
        for r in results:
            new_label = label_map.get(r["label"], r["label"])
            filtered_results.append({
                "originLabel": r["label"],
                "label": new_label,
                "score": round(r["score"], 4)
            })
        best = max(filtered_results, key=lambda x: x["score"])
        new_model_name = model_map.get(model_name, model_name)
        final_results.append({
            "model": new_model_name,
            "top_label": best["label"],
            "top_label_origin": best["originLabel"],
            "top_score": best["score"],
            "details": filtered_results
        })

    return jsonify({
        "status": "finished",
        "results": final_results
    })

@app.route("/vqa-diagnose", methods=["POST"])
def vqa_diagnose():
    model_name = request.form.get("model")
    question = request.form.get("question")
    file = request.files.get("file")

    print(file.filename)

    if not file or not model_name or not question:
        return "Invalid", 400
    
    ext = os.path.splitext(file.filename)[1].lower()
    
    file.save(LOCAL_IMAGE + ext)

    # ===== COMMAND =====
    curl_cmd = f"""
    cd {REMOTE_DIR} && curl -X POST http://10.200.1.4:5000/medgemma -F "image=@upload{ext}" -F "question={question}"
    """

    # ===== INPUT SECRETS =====
    key_passphrase = getpass.getpass("üîë Enter SSH key passphrase: ")
    gpu_password = getpass.getpass("üîê Enter GPU password: ")

    # ===== SSH TO CMS =====
    print("[*] Connecting to CMS...")
    pkey = paramiko.RSAKey.from_private_key_file(
        KEY_PATH, password=key_passphrase
    )

    cms = paramiko.SSHClient()
    cms.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    cms.connect(CMS_HOST, username=USER, pkey=pkey)

    # ===== TUNNEL TO GPU =====
    transport = cms.get_transport()
    dest_addr = (GPU_HOST, 22)
    local_addr = ("127.0.0.1", 0)

    channel = transport.open_channel(
        "direct-tcpip", dest_addr, local_addr
    )

    gpu = paramiko.SSHClient()
    gpu.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    gpu.connect(
        GPU_HOST,
        username=USER,
        password=gpu_password,
        sock=channel
    )

    # ===== UPLOAD IMAGE =====
    print("[*] Uploading image to GPU node...")
    sftp = gpu.open_sftp()

    try:
        sftp.chdir(REMOTE_DIR)
    except IOError:
        sftp.mkdir(REMOTE_DIR)
        sftp.chdir(REMOTE_DIR)

    sftp.put(LOCAL_IMAGE + ext, REMOTE_IMAGE + ext)
    sftp.close()

    # ===== EXEC CURL =====
    print("[*] Running MedGemma request...")
    stdin, stdout, stderr = gpu.exec_command(curl_cmd)

    #gpu.close()
    #cms.close()
    return jsonify({"answer": markdown.markdown(json.loads(stdout.read().decode())['result'])}),200

    # messages = [
    #     {
    #         "role": "system",
    #         "content": [{"type": "text", "text": "You are an expert radiologist."}]
    #     },
    #     {
    #         "role": "user",
    #         "content": [
    #             {"type": "text", "text": question},
    #             {"type": "image", "image": image}
    #         ]
    #     }
    # ]

    # inputs = processor.apply_chat_template(
    #     messages, add_generation_prompt=True, tokenize=True,
    #     return_dict=True, return_tensors="pt"
    # ).to(model.device, dtype=torch.bfloat16)

    # input_len = inputs["input_ids"].shape[-1]

    # with torch.inference_mode():
    #     generation = model.generate(**inputs, max_new_tokens=500, do_sample=False)
    #     generation = generation[0][input_len:]

    # decoded = processor.decode(generation, skip_special_tokens=True)
    # return jsonify({"answer": decoded}),200

    return jsonify({"answer":"T√≠nh nƒÉng n√†y t·∫°m th·ªùi b·ªã ƒë√≥ng do ch∆∞a c√≥ GPU =))"}),200


if __name__=="__main__":
    app.run(host="0.0.0.0", port=8080, debug=True)
